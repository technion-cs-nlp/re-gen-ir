<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>How Generative IR Retrieves Documents Mechanistically</title>
  <link rel="stylesheet" href="mini-default.min.css">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    header {
            background-color: #2E5F7F;
            color: #fff;
            padding: 60px 0;
            text-align: center;
            height: 100%;
        }
        body {
        background-color: #eee;
    }
    .container {
    display: flex;
    justify-content: center;
    }
    .card.wide {
        min-width: 80%;
    }
    table:not(.horizontal) {
        overflow: unset;
        max-height: fit-content;
        max-width: fit-content;
    }
  </style>
</head>
<body>
	<header>
		<h1>How Generative IR Retrieves Documents Mechanistically</h1>
		<address style="margin: 0.5rem;">
			<nobr><a href="https://anja.re/" target="_blank" style="color: #fff" >Anja Reusch</a>,</nobr>
            <nobr><a href="https://belinkov.com/" target="_blank" style="color: #fff">Yonatan Belinkov</a></nobr>
			<br>
			<nobr><institute>Technion - IIT</institute></nobr>;
		</address>
		<a href="" target="_blank" class="button" style="color: #fff; background-color: #77131A;"><i class="ai ai-arxiv"></i> ArXiv</a>
        <a href="https://huggingface.co/AnReu/DSI-large-TriviaQA-QG" target="_blank" class="button" style="color: #fff; background-color: #6093D3	;"><i class="fab fa-huggingface"></i> Models</a>
		<a href="" target="_blank" class="button" style="color: #fff; background-color: #559384;"><i class="far fa-file-pdf"></i> PDF</a>
		<a href="https://github.com/technion-cs-nlp/re-gen-ir" target="_blank" class="button" style="color: #fff; background-color: #EF8933; border-color: #212529;"><i class="fab fa-github"></i> Code</a>
	</header>

<div class="container">
        <div class="card wide">
            <div class="section">
                <h3>Abstract</h3>
                <div class="row">
                    
                    <div class="col-sm-12 col-md-7 col-lg-7">
                        
                        Generative Information Retrieval (GenIR) is a novel paradigm in which a transformer encoder-decoder model predicts document rankings based on a query in an end-to-end fashion. These GenIR models have received significant attention due to their simple retrieval architecture while maintaining high retrieval effectiveness.
                        However, in contrast to established retrieval architectures like cross-encoders or bi-encoders, their internal computations remain largely unknown. Therefore, this work studies the internal retrieval process of GenIR models by applying methods based on mechanistic interpretability, such as patching and vocabulary projections. 
                       <br> 
                       By replacing the GenIR encoder with one trained on fewer documents, we demonstrate that the decoder is the primary component responsible for successful retrieval. Our patching experiments reveal that not all components in the decoder are crucial for the retrieval process. More specifically, we find that a pass through the decoder can be divided into three stages:
                        (I) the priming stage, which contributes important information for activating subsequent components in later layers; (II) the bridging stage, where cross-attention is primarily active to transfer query information from the encoder to the decoder; and (III) the interaction stage, where predominantly MLPs are active to predict the document identifier. 
                        <br>
                        Our findings indicate that interaction between query and document information occurs only in the last stage.
                        We hope our results promote a better understanding of GenIR models and foster future research to overcome the current challenges associated with these models.    
                    </div>
                    <div class="col-sm-12 col-md-5 col-lg-5">
                        <figure>
                        <img src="images/overview.pdf" style="width: 100%; height: auto;" alt="A flow through the transformer encoder-decoder, where the three stages that we find in our study are depicted."/>
                        <figcaption>A simplified view of the retrieval process in the
                            Generative IR models in our work. After the encoder processes the query, the decoder operates in three stages: (I) the
                            Priming Stage, (II) the Bridging Stage, and (III) the Interaction Stage.</figcaption>
                    </figure>
                    </div>
                </div>
            </div>
        </div>
</div>

<div class="container">
        <div class="card wide">
            <div class="section"><h3>Generative Information Retrieval</h3>
                <div class="row">
                    <div class="col-sm-12 col-md-5 col-lg-5">
                        <figure>       
                            <img src="images/gen-ir-residual-stream.pdf" style="width: 100%; height: auto;" alt="A diagram depicting the pass through an encoder-decoder model illustrated as the residual stream. In each step, a component reads from the residuals, processes the input, and writes the output back to the residual."/>
                            <figcaption>Overview of a transformer encoder-decoder for GenIR, depicted as the residual stream to which components
                                read and write.</figcaption>
                        </figure>
                    </div>
                    <div class="col-sm-12 col-md-7 col-lg-7">
                        Generative Information Retrieval (GenIR) trains a transformer encoder-decoder model (depicted left) 
                        to associate a query with a document identifier that satisfies the information need expressed 
                        within the query. During training, the model is presented with either the first N token of document D
                        or a query associated with document D. The training optimizes the model such that it predicts the document
                        identifier for D. Both training objectives (N token -> D or query -> D) are mixed in a multi-task fashion.
                        During inference, the model receives a query as its input and is applied to predict 
                        a document identifier. <br>
                        In this work, we apply <i>atomic</i> GenIR models, meaning that each document identifier is represented 
                        by one token. We expand the vocabulary of the transformer encoder-decoder by the amount of documents 
                        in our corpus.
                        <br>
                        We adopt the common view of the transformer model as a residual stream. In this view, 
                        components such as MLPs or attention "read" information from the stream, i.e., their input is the 
                        residual stream and the "write" back to the residual stream by adding their output to it. On the left,
                        the transformer encoder-decoder model is depicted in this view. MLP and self-attention read from the 
                        residual stream of the encoder and write back to it. In the decoder, MLP and self-attention perform the 
                        same computations. Cross-attention serves as a bridge between the encoder and the decoder. It reads the residual 
                        stream of the decoder as its query vectors and uses the output of the encoder as its key/value vectors. 
                    </div>
                </div>
            </div>
        </div>
</div>

<div class="container">
    <div class="card wide">
        <div class="section"><h3>The Role of the Encoder in GenIR</h3>
            <b>Intuition.</b>
            We investigate whether information on the document corpus are contained in the encoder or only in the decoder after training a GenIR model. We analyze that by evaluating whether a document can still be retrieved even though the encoder was not trained on it.  
            <br>
            <b>Experiments.</b>
            We start by training five GenIR Models: one with the entire dataset (NQ10k), and four models with each 10 different documents less (sampled from those that the full model retrieved successfully).
            Then, we use the encoder of these "partial" models together with the decoder of the full model, creating four models RD I to RD IV. 
            The encoder of these models was not trained on 10 documents, but the document saw these (4*10=) 40 documents during training.

            In addition, we perform an even more drastic experiment in which we replace the trained GenIR encoder with a vanilla pre-trained T5 decoder.
            <br><b>Results.</b>
            The four RD models place the ten documents that their encoder was not trained on on high ranks, in 70 to 80% on Rank 1. Their MRR for these ten documents is between 0.75 and 0.87.
            When using the T5 encoder with the GenIR decoder, the average rank of previously relevant documents drops from 1 to 21.66.
            For most queries, the relevant document is still placed at Rank 1.
            Only a few documents are ranked higher than 1000.
            <br><b>Conclusion.</b>
            Since the models in both experiments are still able to retrieve the
            documents even though their encoder was not trained to retrieve them, we conclude that the documents are not exclusively encoded
            in the encoder. Potentially, the encoder’s role lies in semantically encoding the query whose information is then moved to the decoder. Thus, the decoder holds the key for performing retrieval.
        </div>
    </div>
</div>

<div class="container">
    <div class="card wide">
        <div class="section"><h3>Which Components are Crucial for Retrieval</h3>
        </div>
        <div class="section"><b>How much does each component contribute to the residual stream?</b> We plot the length and the angle of each component (MLP, self-attention, cross-attention) across the pass through the decoder.
            <figure>
                <img src="images/ratio_cosine_5models.pdf" class="media section">
                <figcaption>Length (normalized to the contribution per layer) and angle (towards the residual stream in this layer) of the output of each component in each <i>decoder</i> layer for five models.</figcaption>
            </figure>
        <div>
            <b>Conclusion.</b> We identify three stages during a pass through the decoder when looking at the contribution of each components:
            <ul>
                <li>Stage I: High contribution of MLPs, low to no contribution of Cross-Attention and Self-Attention,</li>
                <li>Stage II: Contribution of Cross-Attention raises, while contribution of MLP declines.</li>
                <li>Stage III: Contribution of MLP is highest, output of all components is directed to the opposite direction than the residual stream.</li>
              </ul>
        </div>
        </div>

        <div class="section">
        <b>How much does it hurt to remove/ replace a component?</b> We perform zero-patching and mean patching on each component in each stage. In each run, we replace the output of a component in one stage with a zero vector or the mean vector of the output of that component aggregated over all queries for which the model previously ranked a relevant document on rank 1.        
        <br>
        <figure class="section">
        <table><thead>
            <tr>
                <th></th>
                <th></th>
                <th>Zero</th>
                <th></th>
                <th></th>
                <th>Mean</th>
                <th></th>
                <th></th>
                <th>T5</th>
                <th></th>
            <tr>
              <th></th>
              <th>Self.-Attn.</th>
              <th>MLP</th>
              <th>Cr.-Attn.</th>
              <th>Self.-Attn.</th>
              <th>MLP</th>
              <th>Cr.-Attn.</th>
              <th>Self.-Attn.</th>
              <th>MLP</th>
              <th>Cr.-Attn.</th>
            </tr>
            
        </thead>
        <tbody>
            <tr>
              <td data-label="Stage">Stage I</td>
              <td data-label="Self.Attn. Zero">1.52</td>
              <td data-label="MLP Zero"><b>92.86</b></td>
              <td data-label="Cr.Attn. Zero">4.29</td>
              <td data-label="Self.Attn. Mean">0.00</td>
              <td data-label="MLP Mean">0.76</td>
              <td data-label="Cr.Attn. Mean">4.55</td>
              <td data-label="Self.Attn. T5">5.30</td>
              <td data-label="MLP T5">5.30</td>
              <td data-label="Cr.Attn. T5">7.58</td>
            </tr>
            <tr>
              <td data-label="Stage">Stage II</td>
              <td data-label="Self.Attn. Zero">2.27</td>
              <td data-label="MLP Zero">7.83</td>
              <td data-label="Cr.Attn. Zero"><b>21.72</b></td>
              <td data-label="Self.Attn. Mean">2.02</td>
              <td data-label="MLP Mean">5.56</td>
              <td data-label="Cr.Attn. Mean">20.45</td>
              <td data-label="Self.Attn. T5">5.56</td>
              <td data-label="MLP T5">7.07</td>
              <td data-label="Cr.Attn. T5">17.93</td>
            </tr>
            <tr>
              <td data-label="Stage">Stage III</td>
              <td data-label="Self.Attn. Zero">6.57</td>
              <td data-label="MLP Zero"><b>96.72</b></td>
              <td data-label="Cr.Attn. Zero">39.65</td>
              <td data-label="Self.Attn. Mean">6.06</td>
              <td data-label="MLP Mean">48.23</td>
              <td data-label="Cr.Attn. Mean">42.93</td>
              <td data-label="Self.Attn. T5">7.32</td>
              <td data-label="MLP T5">74.49</td>
              <td data-label="Cr.Attn. T5">34.09</td>
            </tr>
          </tbody>
          </table>
          <figcaption>Percent of correct queries where the rank of the relevant document is not on Rank 1 after applying patching, we remove/ replace the indicated component output only in the indicated stage. The highest value, i.e., the largest drop in performance per model and stage is indicated bold. MLPs in Stage I can be replaced by mean activations, cross-attention in Stage II and III is crucial for the retrieval process, and MLPs in Stage III are the components adapted most for retrieval. Results for NQ10k.</figcaption>
        </figure>

        <br>
        <b>
            Conclusion.
        </b>
        The results verify our intution gained from the pervious part. In Stage I and III, MLPs cannot be removed. In Stage II, Cross-Attention shows the highest impact when being removed/ replaced. Interestingly, replacing the MLP output in Stage I and II by their mean values does not seem to hurt performance drastically. This implies that they do not perform query specific computations.
        
        </div>
        <div class="section">
        <b>How well does a "minimal" model perform retrieval?</b> We remove all "unnecessary" components and run this minimal model consisting of only the most crucial components on our testset. The minimal model consists of: Only mean-output of MLPs in Stage I, Cross-Attention output and mean-output of MLPs in Stage II, and Cross-Attention and MLP output in Stage III. Self-Attention is removed from the entire model.
        <br>
        The results (see our paper for details) show that this minimal model can retain 78% to 98% faithfulness depending on the dataset and metric. That means, for example, for R@5 the model trained on NQ10k retains 98% of its "full model" performance when using only this minimal set of components.
        </div>
        
    </div>
</div>


<div class="container">
    <div class="card wide">
        <div class="section"><h3>The Role of MLPs and Cross-Attention</h3>
        </div>
        <div class="section">
            <b>Which component influences the rank of the relevant document?</b> We use <a href="https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens" target="_blank">LogitLens</a> to project the residual stream and the output of MLPs and Cross-Attention to the space of the logit. This way, we can identify which information get "written" to the residual stream by which component. 
            <figure>
                <img src="images/ranks_5models.pdf" class="media section">
                <figcaption>Development of the ranks of the (first) relevant document for five different datasets, displayed per decoder layer. The models separate document identifiers and non-document-identifiers early, and gradually improve the rank of the relevant document. The cross-attention output does not seem to follow this gradual progression. </figcaption>
            </figure>
            <b>Conclusion.</b> Since the MLP output places the relevant document quickly in low ranks, we assume that this is the main component reponsible for writing document information to the residual stream. Cross-Attention also writes "useful" information (we know that because we saw in the previous section that it hurts performance if we remove/ replace it, but, it also makes sense that Cross-Attention is somewhat important, since it is the only component that moves information from the encoder to the decoder.), but these information are apparently not directly used by the model (otherwise it would influence the logits more). Instead we assume that Cross-Attention writes some information to the residual stream, which MLPs read, process, and output to the logits.
        </div>
    
        <div class="section">
            <div class="row">
                    
                <div class="col-sm-12 col-md-7 col-lg-7">
                    <b>Do Cross-Attention and MLPs communicate?</b> We would like to investigate whether our intuition that cross-attention writes something to the residual stream, that is read in by the MLPs is correct. We therefore look at the components that activate the neurons (neurons are the sub-components that MLPs are composed of). For a given query, certain neurons activate. Our goal is to determine which model components before these neurons cause them to activate. This is done by computing the cosine similarity of each components output with the LL_in vector of that neuron. This yields a score of how much a certain component lead to the activation of a neuron, or if we aggregate over all neurons of one MLP, how much a certain component influences the output of an MLP. We can then aggregate the component scores by stage. We perform the same computations and aggregate for Cross-Attention. This provides us with an understanding of which components "activate" ("trigger" in the paper) a cross-attention head and, thereby, influence the output of a cross-attention component. 
                </div>
                <div class="col-sm-12 col-md-5 col-lg-5">
                    <figure>
                        <img src="images/cr_attn_neuron_reading_10k_old_colors.pdf" style="width: 100%; height: auto;">
                        <figcaption>Components per stage that trigger cross-attention in Stage II and III (left) and activate MLPs in Stage III (right) of NQ10k. Stage III MLPs gets mostly activated from cross-attention in Stage II and III, while cross-attention in Stage II gets mostly activated by Stage I MLPs.</figcaption>
                    </figure>
                </div>
            </div>
            <b>Conclusion.</b> The large green proportion in the MLP plot tells us that the MLPs in these layers are mostly activated by the output of Cross-Attention components in Stage II, and later in Stage III. Attention and MLP outputs are less important for these MLPs. However, in the last layer, MLP output from Stage I seems also to activate neurons in this layer. In addition, we can view in the left plot which components in which stages activate/ "trigger" Cross-Attention components. Here we see a large proportion taken up by the MLPs in Stage I. Later the proportion of Stage II MLPs and also Cross-Attention increases. 
            <br>Our findings suggest that MLPs in Stage I write information to the residual stream that used to trigger Cross-Attention in Stage II. This information is query-agnostic as it can be replaced by the mean output of these components. This leads us to denote Stage I by <b>Priming Stage</b>. 
        </div>
        <div class="section">
            <b>What does Cross-Attention output?</b>
            We apply LogitLens again on the output of the Cross-Attention components to project it to the space of the logits. A logit can either belong to a document identifier token or to a word token (i.e., everything that is not a document identfier which is the vocabulary of the pre-trained model before fine-tuning on GenIR). We then compute the proportion of document-identifier tokens vs. word token which get promoted by the output of the Cross-Attention components.
            The results show that, across all datasets we investigated, in less than one percent document-identifier tokens are within the Top 100 tokens that get promoted by the cross-attention outputs in Stage II. This leads us to the assumption that Cross-Attention writes information in the "word-space" of the model from which MLPs read later on. 
            We can also take a look at what heads in these layers output:
            <table><thead>
                <tr>
                  <th>Query</th>
                  <th>Head</th>
                  <th>Top 5 Words</th>
                </tr></thead>
              <tbody>
                <tr>
                  <td rowspan="2" data-label="Query">who wrote the harry potter books</td>
                  <td data-label="Head" >Layer 14 - Head 2</td>
                  <td data-label="Top 5 Words">about, written, about, tailored, privire</td>
                </tr>
                <tr>
                  <td></td>
                  <td data-label="Head">Layer 16 - Head 1</td>
                  <td data-label="Top 5 Words">books, ouvrage, books, authors, book</td>
                </tr>
                <tr>
                  <td rowspan="2" data-label="Query">who won the football championship in 2002</td>
                  <td data-label="Head">Layer 16 - Head 1</td>
                  <td data-label="Top 5 Words">year, YEAR, Year, year, jahr</td>
                </tr>
                <tr>
                    <td></td>
                  <td data-label="Head">Layer 16 - Head 13</td>
                  <td data-label="Top 5 Words">football, Football, fotbal, soccer, NFL</td>
                </tr>
                <tr>
                  <td rowspan="2" data-label="Query">will there be a sequel to baytown outlaws</td>
                  <td data-label="Head">Layer 12 - Head 8</td>
                  <td data-label="Top 5 Words">erneut, successor, similarly, repris, continuation</td>
                </tr>
                <tr>
                    <td></td>
                  <td data-label="Head">Layer 16 - Head 1</td>
                  <td data-label="Top 5 Words">town, towns, city, Town, village</td>
                </tr>
              </tbody>
              </table>
        </div>
    </div>
</div>

<div class="container">
    <div class="card wide">
        <div class="section">
            <h3>Walkthrough the Retrieval Process</h3>
            <div class="row">
                
                <div class="col-sm-12 col-md-7 col-lg-7">    
                    <b>Encoder.</b> The model begins by embedding the query at the beginning of the encoder. The embedded query vectors are passed through the encoder, which applies MLPs and self-attention to contextualize the input tokens. Our results show that the encoder is not required to encode information on the documents directly as it can be replaced by an encoder that does not contain document specific information (see Sec. 4)
                    <br>
                    <b>Priming Stage.</b>  
                    The decoder receives as its input a generic start
                    token, which also gets embedded in the same way as in the encoder.
                    In the first stage of the model (layers 0–6), no query specific information 
                    is required (Sec. 5.1). Using the MLP components, the model
                    moves document id tokens to lower ranks and non-document-id
                    tokens to higher ranks (Sec. 5.2) while adding information used by
                    subsequent layers to trigger cross-attention.
                    <br>
                    <b>Bridging Stage.</b>
                    In the second stage (layers 7–17), cross-attention
                    moves information from the encoder to the decoder. The cross-
                    attention heads output information on the input query to the residual stream, 
                    which resembles a form of query expansion. This information is then used to 
                    activate neurons in later layers (Sec. 5.3).
                    Our experiments indicate that this communication takes places in
                    the space of word-tokens (Sec. 5.4).
                    <br>
                    <b>Interaction Stage.</b>
                    In Stage III (layers 18–23), cross-attention continues to output query information to the residual stream. At the
                    same time, the MLP neurons are activated and output information that 
                    promotes document identifier tokens (Sec. 5.2). In the
                    last layer, only the MLPs are required. They do not read from the
                    last layer Cross-Attention component (Sec. 5.3). In this layer, all
                    non-document-id tokens are moved to lower ranks, such that only
                    document id tokens will be predicted by the model (Sec. 5.2). This
                    stage is the one that received the greatest adaption to the retrieval
                    task during the GenIR training. This fact implies that corpus specific information 
                    is most probable be stored in components in this
                    stage. Therefore, the query interacts only in this stage with document information. 
                    Finally, in the last layer, the residual stream
                    is multiplied by the unembedding matrix, resulting in logits for
                    each token. Relevant document identifier tokens should receive the
                    highest logits for retrieval to be successful.
                    <br>
                    Even though our results suggest that the same retrieval process
                    is present in all models we examined, differences were observable
                    as well. In models trained on larger datasets, MLPs in the first and
                    second stage seem similarly important, indicating that the priming
                    stage might be longer than seven layers as for NQ10k.
                </div>
                <div class="col-sm-12 col-md-5 col-lg-5">
                    <figure>
                    <img src="images/overview.pdf" style="width: 100%; height: auto;" alt="A flow through the transformer encoder-decoder, where the three stages that we find in our study are depicted."/>
                    <figcaption>A simplified view of the retrieval process in the
                        Generative IR models in our work. After the encoder processes the query, the decoder operates in three stages: (I) the
                        Priming Stage, (II) the Bridging Stage, and (III) the Interaction Stage.</figcaption>
                </figure>
                </div>
            </div>
        </div>
    </div>
</div>

</body>
<footer>
    Made with <a href="https://minicss.us/">minicss</a>.
</footer>
</html>